{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPFPbnjqTPd7/B/naY0DhSL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RAVIPATISRIVIDYA/devtraining-needit-madrid/blob/master/Poisonattack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADht2pzy1YFM",
        "outputId": "06f56e56-cf05-4ee3-bc79-0c6dad73ac1b"
      },
      "source": [
        "!pip install adversarial-robustness-toolbox"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting adversarial-robustness-toolbox\n",
            "  Downloading adversarial_robustness_toolbox-1.8.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox) (4.62.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox) (57.4.0)\n",
            "Collecting numba>=0.53.1\n",
            "  Downloading numba-0.54.1-cp37-cp37m-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 39.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn<1.1.0,>=0.22.2 in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox) (0.22.2.post1)\n",
            "Collecting llvmlite<0.38,>=0.37.0rc1\n",
            "  Downloading llvmlite-0.37.0-cp37-cp37m-manylinux2014_x86_64.whl (26.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.3 MB 83 kB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1.1.0,>=0.22.2->adversarial-robustness-toolbox) (1.1.0)\n",
            "Installing collected packages: llvmlite, numba, adversarial-robustness-toolbox\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.34.0\n",
            "    Uninstalling llvmlite-0.34.0:\n",
            "      Successfully uninstalled llvmlite-0.34.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.51.2\n",
            "    Uninstalling numba-0.51.2:\n",
            "      Successfully uninstalled numba-0.51.2\n",
            "Successfully installed adversarial-robustness-toolbox-1.8.1 llvmlite-0.37.0 numba-0.54.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofL0ECMu1oiu"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "from art.utils import load_dataset\n",
        "from art.utils import load_dataset\n",
        "from art.attacks.poisoning.perturbations.image_perturbations import add_pattern_bd, add_single_bd\n",
        "from art.utils import load_mnist, preprocess,load_cifar10\n",
        "from art.estimators.classification import KerasClassifier\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import art\n",
        "import pprint\n",
        "import json\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k31gt7T-1qvA"
      },
      "source": [
        "def generate_backdoor(\n",
        "    x_clean, y_clean, num_poison, backdoor_type=\"pattern\", sources=np.arange(10), targets=(np.arange(10) + 1) % 10\n",
        "):\n",
        "\n",
        "    max_val = np.max(x_clean)\n",
        "\n",
        "    x_poison = np.copy(x_clean)\n",
        "    y_poison = np.copy(y_clean)\n",
        "    is_poison = np.zeros(np.shape(y_poison))\n",
        "\n",
        "    for i, (src, tgt) in enumerate(zip(sources, targets)):\n",
        "      n_points_in_tgt = np.size(np.where(y_clean == tgt))\n",
        "      num_poison = num_poison #round((percent_poison * n_points_in_tgt) / (1 - percent_poison))\n",
        "      print(num_poison)\n",
        "      \n",
        "      src_imgs = x_clean[np.ravel(y_clean == src)]\n",
        "      # src_imgs = x_clean[y_clean == src]\n",
        "\n",
        "      n_points_in_src = np.shape(src_imgs)[0]\n",
        "      indices_to_be_poisoned = np.random.choice(n_points_in_src, num_poison)\n",
        "\n",
        "      imgs_to_be_poisoned = np.copy(src_imgs[indices_to_be_poisoned])\n",
        "      if backdoor_type == \"pattern\":\n",
        "          imgs_to_be_poisoned = add_pattern_bd(x=imgs_to_be_poisoned, pixel_value=max_val)\n",
        "      elif backdoor_type == \"pixel\":\n",
        "          imgs_to_be_poisoned = add_single_bd(imgs_to_be_poisoned, pixel_value=max_val)\n",
        "      x_poison = np.append(x_poison, imgs_to_be_poisoned, axis=0)\n",
        "      # import pdb\n",
        "      # pdb.set_trace()\n",
        "      y_poison = np.append(y_poison, np.ones((num_poison,1)) * tgt, axis=0)\n",
        "      # y_poison = np.append(y_poison, np.ones(num_poison) * tgt, axis=0)\n",
        "      is_poison = np.append(is_poison, np.ones(num_poison))\n",
        "\n",
        "    is_poison = is_poison != 0\n",
        "\n",
        "    return is_poison, x_poison, y_poison\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdJRhTqZ1tLh"
      },
      "source": [
        "max_val = np.max(x_clean)\n",
        "\n",
        "    x_poison = np.copy(x_clean)\n",
        "    y_poison = np.copy(y_clean)\n",
        "    is_poison = np.zeros(np.shape(y_poison))\n",
        "\n",
        "    for i, (src, tgt) in enumerate(zip(sources, targets)):\n",
        "      n_points_in_tgt = np.size(np.where(y_clean == tgt))\n",
        "      num_poison = num_poison #round((percent_poison * n_points_in_tgt) / (1 - percent_poison))\n",
        "      print(num_poison)\n",
        "      \n",
        "      src_imgs = x_clean[np.ravel(y_clean == src)]\n",
        "      # src_imgs = x_clean[y_clean == src]\n",
        "\n",
        "      n_points_in_src = np.shape(src_imgs)[0]\n",
        "      indices_to_be_poisoned = np.random.choice(n_points_in_src, num_poison)\n",
        "\n",
        "      imgs_to_be_poisoned = np.copy(src_imgs[indices_to_be_poisoned])\n",
        "      if backdoor_type == \"pattern\":\n",
        "          imgs_to_be_poisoned = add_pattern_bd(x=imgs_to_be_poisoned, pixel_value=max_val)\n",
        "      elif backdoor_type == \"pixel\":\n",
        "          imgs_to_be_poisoned = add_single_bd(imgs_to_be_poisoned, pixel_value=max_val)\n",
        "      x_poison = np.append(x_poison, imgs_to_be_poisoned, axis=0)\n",
        "      # import pdb\n",
        "      # pdb.set_trace()\n",
        "      y_poison = np.append(y_poison, np.ones((num_poison,1)) * tgt, axis=0)\n",
        "      # y_poison = np.append(y_poison, np.ones(num_poison) * tgt, axis=0)\n",
        "      is_poison = np.append(is_poison, np.ones(num_poison))\n",
        "\n",
        "    is_poison = is_poison != 0\n",
        "\n",
        "    return is_poison, x_poison, y_poison"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUCzfv3W1zmC",
        "outputId": "388aa1e0-8e86-422f-cf3d-c9eda87fbaf5"
      },
      "source": [
        "(x_raw, y_raw), (x_raw_test, y_raw_test), min_, max_ = load_cifar10(raw=True)\n",
        "n_train = np.shape(x_raw)[0]\n",
        "num_selection = 50000\n",
        "random_selection_indices = np.random.choice(n_train, num_selection)\n",
        "x_raw = x_raw[random_selection_indices]\n",
        "y_raw = y_raw[random_selection_indices]\n",
        "print(x_raw.shape,y_raw.shape)\n",
        "# num_images_to_poison= 2000\n",
        "perc_poison= 0.33#num_images_to_poison/x_raw.shape[0]\n",
        "(is_poison_train, x_poisoned_raw, y_poisoned_raw) = generate_backdoor(x_raw, y_raw, num_poison=1500)\n",
        "# x_train, y_train = preprocess(x_poisoned_raw, y_poisoned_raw)\n",
        "x_train,y_train = x_poisoned_raw/255,to_categorical(y_poisoned_raw,10)\n",
        "# Add channel axis:\n",
        "# x_train = np.expand_dims(x_train, axis=3)\n",
        "# Poison test data\n",
        "(is_poison_test, x_poisoned_raw_test, y_poisoned_raw_test) = generate_backdoor(x_raw_test, y_raw_test, num_poison=100)\n",
        "# x_test, y_test = preprocess(x_poisoned_raw_test, y_poisoned_raw_test)\n",
        "x_test,y_test = x_poisoned_raw_test/255,to_categorical(y_poisoned_raw_test,10)\n",
        "# Add channel axis:\n",
        "# x_test = np.expand_dims(x_test, axis=3)\n",
        "# Shuffle training data so poison is not together\n",
        "n_train = np.shape(y_train)[0]\n",
        "shuffled_indices = np.arange(n_train)\n",
        "np.random.shuffle(shuffled_indices)\n",
        "x_train = x_train[shuffled_indices]\n",
        "y_train = y_train[shuffled_indices]\n",
        "is_poison_train = is_poison_train[shuffled_indices]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 32, 32, 3) (50000, 1)\n",
            "1500\n",
            "1500\n",
            "1500\n",
            "1500\n",
            "1500\n",
            "1500\n",
            "1500\n",
            "1500\n",
            "1500\n",
            "1500\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-S37IHin13nt",
        "outputId": "96b24646-8e21-47ff-b9f4-6a786d83c3d6"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation=\"relu\", input_shape=x_train.shape[1:]))\n",
        "model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "classifier = KerasClassifier(model=model, clip_values=(min_, max_))\n",
        "\n",
        "classifier.fit(x_train, y_train, nb_epochs=30, batch_size=128)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 65000 samples\n",
            "Epoch 1/30\n",
            "65000/65000 [==============================] - 11s 169us/sample - loss: 1.7327 - accuracy: 0.3637\n",
            "Epoch 2/30\n",
            "65000/65000 [==============================] - 11s 164us/sample - loss: 1.3526 - accuracy: 0.5159\n",
            "Epoch 3/30\n",
            "65000/65000 [==============================] - 11s 163us/sample - loss: 1.1859 - accuracy: 0.5797\n",
            "Epoch 4/30\n",
            "65000/65000 [==============================] - 11s 162us/sample - loss: 1.0647 - accuracy: 0.6256\n",
            "Epoch 5/30\n",
            "65000/65000 [==============================] - 11s 162us/sample - loss: 0.9877 - accuracy: 0.6535\n",
            "Epoch 6/30\n",
            "65000/65000 [==============================] - 11s 162us/sample - loss: 0.9135 - accuracy: 0.6767\n",
            "Epoch 7/30\n",
            "65000/65000 [==============================] - 11s 163us/sample - loss: 0.8595 - accuracy: 0.6988\n",
            "Epoch 8/30\n",
            "65000/65000 [==============================] - 11s 162us/sample - loss: 0.8011 - accuracy: 0.7152\n",
            "Epoch 9/30\n",
            "65000/65000 [==============================] - 11s 162us/sample - loss: 0.7560 - accuracy: 0.7324\n",
            "Epoch 10/30\n",
            "65000/65000 [==============================] - 10s 161us/sample - loss: 0.7220 - accuracy: 0.7408\n",
            "Epoch 11/30\n",
            "65000/65000 [==============================] - 10s 161us/sample - loss: 0.6881 - accuracy: 0.7536\n",
            "Epoch 12/30\n",
            "65000/65000 [==============================] - 10s 161us/sample - loss: 0.6570 - accuracy: 0.7631\n",
            "Epoch 13/30\n",
            "65000/65000 [==============================] - 11s 162us/sample - loss: 0.6293 - accuracy: 0.7732\n",
            "Epoch 14/30\n",
            "65000/65000 [==============================] - 10s 161us/sample - loss: 0.6044 - accuracy: 0.7826\n",
            "Epoch 15/30\n",
            "65000/65000 [==============================] - 10s 161us/sample - loss: 0.5787 - accuracy: 0.7904\n",
            "Epoch 16/30\n",
            "65000/65000 [==============================] - 10s 161us/sample - loss: 0.5751 - accuracy: 0.7918\n",
            "Epoch 17/30\n",
            "65000/65000 [==============================] - 10s 161us/sample - loss: 0.5538 - accuracy: 0.8005\n",
            "Epoch 18/30\n",
            "65000/65000 [==============================] - 10s 161us/sample - loss: 0.5335 - accuracy: 0.8068\n",
            "Epoch 19/30\n",
            "65000/65000 [==============================] - 11s 162us/sample - loss: 0.5217 - accuracy: 0.8093\n",
            "Epoch 20/30\n",
            "65000/65000 [==============================] - 10s 161us/sample - loss: 0.5033 - accuracy: 0.8157\n",
            "Epoch 21/30\n",
            "65000/65000 [==============================] - 10s 161us/sample - loss: 0.4989 - accuracy: 0.8185\n",
            "Epoch 22/30\n",
            "65000/65000 [==============================] - 10s 161us/sample - loss: 0.4800 - accuracy: 0.8248\n",
            "Epoch 23/30\n",
            "65000/65000 [==============================] - 10s 161us/sample - loss: 0.4697 - accuracy: 0.8253\n",
            "Epoch 24/30\n",
            "65000/65000 [==============================] - 11s 162us/sample - loss: 0.4676 - accuracy: 0.8297\n",
            "Epoch 25/30\n",
            "65000/65000 [==============================] - 11s 162us/sample - loss: 0.4541 - accuracy: 0.8326\n",
            "Epoch 26/30\n",
            "65000/65000 [==============================] - 11s 162us/sample - loss: 0.4515 - accuracy: 0.8338\n",
            "Epoch 27/30\n",
            "65000/65000 [==============================] - 11s 162us/sample - loss: 0.4442 - accuracy: 0.8365\n",
            "Epoch 28/30\n",
            "65000/65000 [==============================] - 11s 163us/sample - loss: 0.4341 - accuracy: 0.8399\n",
            "Epoch 29/30\n",
            "65000/65000 [==============================] - 10s 161us/sample - loss: 0.4278 - accuracy: 0.8426\n",
            "Epoch 30/30\n",
            "65000/65000 [==============================] - 10s 161us/sample - loss: 0.4206 - accuracy: 0.8453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaQf03G716pz",
        "outputId": "3ea9190b-8d6c-4c4c-9dc0-9fd7fa0d6115"
      },
      "source": [
        "preds = np.argmax(classifier.predict(x_test), axis=1)\n",
        "acc = np.sum(preds == np.argmax(y_test, axis=1)) / y_test.shape[0]\n",
        "print(\"\\nTest accuracy: %.2f%%\" % (acc * 100))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test accuracy: 67.89%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XO_Pz-cA19Sz",
        "outputId": "e4a29399-162f-497f-9b28-daadeb444295"
      },
      "source": [
        "preds = np.argmax(classifier.predict(x_test[is_poison_test]), axis=1)\n",
        "acc = np.sum(preds == np.argmax(y_test[is_poison_test], axis=1)) / y_test[is_poison_test].shape[0]\n",
        "print(\"\\nPoisonous test set accuracy (i.e. effectiveness of poison): %.2f%%\" % (acc * 100))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Poisonous test set accuracy (i.e. effectiveness of poison): 61.80%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvlknH8z1_sT",
        "outputId": "e6bda40e-1a73-46e8-c04b-7680b09a7f0d"
      },
      "source": [
        "preds = np.argmax(classifier.predict(x_test[is_poison_test == 0]), axis=1)\n",
        "acc = np.sum(preds == np.argmax(y_test[is_poison_test == 0], axis=1)) / y_test[is_poison_test == 0].shape[0]\n",
        "print(\"\\nClean test set accuracy: %.2f%%\" % (acc * 100))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Clean test set accuracy: 68.50%\n"
          ]
        }
      ]
    }
  ]
}