{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "extraction.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RAVIPATISRIVIDYA/devtraining-needit-madrid/blob/master/extractionattacks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXhz4z6XiAUI"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore',category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "import tensorflow as tf\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as img\n",
        "%matplotlib inline\n",
        "import tensorflow_hub as thub\n",
        "import tensorflow.keras.backend as K"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sl7BlbcvvISU",
        "outputId": "c9b0fd11-c085-446f-9daa-093eb02d1ad6"
      },
      "source": [
        "!pip install adversarial-robustness-toolbox\n",
        "import art"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting adversarial-robustness-toolbox\n",
            "  Downloading adversarial_robustness_toolbox-1.8.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 16.2 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 17.2 MB/s eta 0:00:01\r\u001b[K     |█                               | 30 kB 19.4 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 17.8 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 61 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 92 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 122 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 153 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 184 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████                          | 204 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 215 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 235 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 245 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 266 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 276 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 296 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 307 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 317 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 327 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 337 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 358 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 368 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 378 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 389 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 399 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 409 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 419 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 430 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 440 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 460 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 471 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 481 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 491 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 501 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 512 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 522 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 532 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 542 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 552 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 563 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 573 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 583 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 593 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 604 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 614 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 634 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 645 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 655 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 665 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 675 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 686 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 696 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 706 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 716 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 727 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 737 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 747 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 757 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 768 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 778 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 788 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 798 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 808 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 819 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 829 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 839 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 849 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 860 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 870 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 880 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 890 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 901 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 911 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 921 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 931 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 942 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 952 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 962 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 972 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 983 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 993 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0 MB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.0 MB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0 MB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0 MB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.0 MB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1 MB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.1 MB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1 MB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.1 MB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1 MB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox) (57.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox) (4.62.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn<1.1.0,>=0.22.2 in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox) (0.22.2.post1)\n",
            "Collecting numba>=0.53.1\n",
            "  Downloading numba-0.54.1-cp37-cp37m-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 22.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox) (1.15.0)\n",
            "Collecting llvmlite<0.38,>=0.37.0rc1\n",
            "  Downloading llvmlite-0.37.0-cp37-cp37m-manylinux2014_x86_64.whl (26.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.3 MB 90 kB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1.1.0,>=0.22.2->adversarial-robustness-toolbox) (1.1.0)\n",
            "Installing collected packages: llvmlite, numba, adversarial-robustness-toolbox\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.34.0\n",
            "    Uninstalling llvmlite-0.34.0:\n",
            "      Successfully uninstalled llvmlite-0.34.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.51.2\n",
            "    Uninstalling numba-0.51.2:\n",
            "      Successfully uninstalled numba-0.51.2\n",
            "Successfully installed adversarial-robustness-toolbox-1.8.1 llvmlite-0.37.0 numba-0.54.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RycPWMYFxC3R"
      },
      "source": [
        "from art.estimators.classification import KerasClassifier\n",
        "from art.attacks.evasion import FastGradientMethod\n",
        "from art.attacks.extraction import CopycatCNN, FunctionallyEquivalentExtraction, KnockoffNets"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcwL4s2bxSPM"
      },
      "source": [
        "def sample_by_class(data, labels, num_samples=100):\n",
        "    sample_data = []\n",
        "    sample_labels = []\n",
        "    unq_labels = list(range(labels.shape[1]))\n",
        "    for label in unq_labels:\n",
        "        idx = labels[:,label]==1\n",
        "        sample_set = data[idx][0:num_samples].copy()\n",
        "        label_set = labels[idx][0:num_samples].copy()\n",
        "        sample_data.append(sample_set)\n",
        "        sample_labels.append(label_set)\n",
        "    \n",
        "    sample_data = np.concatenate(sample_data)\n",
        "    sample_labels = np.concatenate(sample_labels)\n",
        "    print(sample_data.shape, sample_labels.shape)\n",
        "    return sample_data, sample_labels\n",
        "\n",
        "def subset_data(data, labels, fraction=5):\n",
        "    data_size = data.shape[0]\n",
        "    out_size = int(data_size*fraction/100)\n",
        "    idx = np.random.choice(data_size, out_size, replace=False)\n",
        "    out_data = data[idx].copy()\n",
        "    out_labels = labels[idx].copy()\n",
        "    \n",
        "    print(out_data.shape, out_labels.shape)\n",
        "    return out_data, out_labels"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeTy5dlRxWgo",
        "outputId": "62cbcacf-db8b-44dc-a56a-08061e19e4b4"
      },
      "source": [
        "from tensorflow import keras\n",
        "(x_train,y_train),(x_test,y_test) = keras.datasets.cifar10.load_data()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 3s 0us/step\n",
            "170508288/170498071 [==============================] - 3s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHehKyhox-4N",
        "outputId": "b694fd83-f938-47f3-e824-a66b7c162388"
      },
      "source": [
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhK_hMLQyDJn"
      },
      "source": [
        "testd = x_train[0:1000].copy()\n",
        "testl = y_train[0:1000].copy()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoqAiZIXyGSC"
      },
      "source": [
        "def build_ganeval_model():\n",
        "    K.clear_session()\n",
        "    model_url = \"https://tfhub.dev/deepmind/ganeval-cifar10-convnet/1\"\n",
        "    ganeval_module = thub.Module(model_url)\n",
        "    \n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.InputLayer(input_shape=(32,32,3)))\n",
        "    model.add(thub.KerasLayer(ganeval_module))\n",
        "    model.add(tf.keras.layers.Activation('softmax'))\n",
        "    \n",
        "    model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.1, momentum=0.9, decay= 1e-4),\n",
        "                  loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q09qWquByLBx"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "K.clear_session()\n",
        "model_url = \"https://tfhub.dev/deepmind/ganeval-cifar10-convnet/1\"\n",
        "ganeval_module = thub.Module(model_url)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwhGiU4a0dI1",
        "outputId": "5355fdd1-952d-4137-aafc-05d96f22a699"
      },
      "source": [
        "ge_cifar_clf = tf.keras.Sequential()\n",
        "ge_cifar_clf.add(tf.keras.layers.InputLayer(input_shape=(32,32,3)))\n",
        "# gan_eval_model.add(gan_eval_layer)\n",
        "ge_cifar_clf.add(thub.KerasLayer(ganeval_module))\n",
        "ge_cifar_clf.add(tf.keras.layers.Activation('softmax'))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDIF_Ufa02rS",
        "outputId": "ab08f7de-de78-4456-ce50-6053c6d0ac92"
      },
      "source": [
        "ge_cifar_clf.compile(optimizer=tf.keras.optimizers.SGD(lr=0.1, momentum=0.9, decay=1e-4),\n",
        "                       loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "                       metrics=['accuracy'])\n",
        "ge_cifar_clf.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer (KerasLayer)    (None, 10)                7796426   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,796,426\n",
            "Trainable params: 0\n",
            "Non-trainable params: 7,796,426\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvG2ZfBS02uo",
        "outputId": "62242101-8807-43e3-af0a-966361ac6fea"
      },
      "source": [
        "ge_cifar_clf = build_ganeval_model()\n",
        "ge_cifar_clf.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer (KerasLayer)    (None, 10)                7796426   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,796,426\n",
            "Trainable params: 0\n",
            "Non-trainable params: 7,796,426\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArI9nruj1B6X",
        "outputId": "cfd9bcd7-b7ee-4669-c3bd-24c4b9f3be93"
      },
      "source": [
        "ge_cifar_clf.evaluate(testd, testl)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2057: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[166.622517578125, 0.11]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtyOoZwK1Jme"
      },
      "source": [
        "classifier = KerasClassifier(model=ge_cifar_clf, clip_values=(0, 1), use_logits=False)\n",
        "attack_fgsm = FastGradientMethod(estimator=classifier, eps=0.4)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNrPLKPB1Ze0",
        "outputId": "21349472-8684-4f7b-b6ec-a89068863871"
      },
      "source": [
        "ge_cifar_clf.evaluate(testd, testl)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[166.622517578125, 0.11]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdayPSh59hHu"
      },
      "source": [
        "classifier = KerasClassifier(model=ge_cifar_clf, clip_values=(0, 1), use_logits=True)\n",
        "attack_fgsm = FastGradientMethod(estimator=classifier, eps=0.4)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8uZyxJB9laJ",
        "outputId": "5135df8c-638e-481e-ebe2-7f93d643bfd0"
      },
      "source": [
        "x_test_adv = attack_fgsm.generate(testd.astype(np.float32).copy())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWbG7PyBEC21",
        "outputId": "f7b8d4c5-ff59-4d1d-ebd4-0479acd0f4e3"
      },
      "source": [
        "ge_cifar_clf.evaluate(x_test_adv, testl)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[166.089640625, 0.11]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8vXo8-nEs4Q"
      },
      "source": [
        "def build_substitute_model(enable_logits=True):\n",
        "    model = tf.keras.Sequential( )\n",
        "    model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=(32,32,3)))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Conv2D(32, (3, 3)))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(keras.layers.Dropout(0.25))\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Conv2D(64, (3, 3)))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(10))\n",
        "    if enable_logits:\n",
        "        model.add(Activation(\"linear\"))\n",
        "    else:\n",
        "        model.add(Activation(\"softmax\"))\n",
        "        \n",
        "    model.compile(loss=CategoricalCrossentropy(from_logits=enable_logits),\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'] )\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGGP7QbQFLnw",
        "outputId": "ca225f33-29e2-41ab-b283-f601e7352b17"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D\n",
        "from keras.layers import Activation, Dense\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D,Dropout,Flatten\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "sub_model = build_substitute_model()\n",
        "sub_model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 32, 32, 32)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 30, 30, 32)        9248      \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 30, 30, 32)        0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 15, 15, 32)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 15, 15, 64)        18496     \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 15, 15, 64)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 13, 13, 64)        36928     \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 13, 13, 64)        0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 6, 6, 64)          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2304)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               1180160   \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 512)               0         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,250,858\n",
            "Trainable params: 1,250,858\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt8-PXFWJJpL"
      },
      "source": [
        "def extraction_attack(ART_attack, x_train, y_train, fractions, epoch=30, verbose=0):\n",
        "    \n",
        "    data_size = x_train.shape[0]\n",
        "    epochs = epoch\n",
        "    \n",
        "    ### Train and Test sample data from each class to get final results\n",
        "    print(\"Extract 100 Training samples from each class:\")\n",
        "    x_train_adv, y_train_adv = sample_by_class(x_train, y_train, num_samples=100)\n",
        "    print(\"Extract 100 Test samples from each class:\")\n",
        "    x_test_adv, y_test_adv = sample_by_class(x_test, y_test, num_samples=100)\n",
        "    \n",
        "    ge_cifar_clf = build_ganeval_model()\n",
        "    cloud_art_clf = KerasClassifier(model=ge_cifar_clf, clip_values=(0, 1), use_logits=True)\n",
        "    \n",
        "    loss1, acc1 = cloud_art_clf._model.evaluate(x_train_adv, y_train_adv, verbose=verbose)\n",
        "    loss2, acc2 = cloud_art_clf._model.evaluate(x_test_adv, y_test_adv, verbose=verbose)\n",
        "    \n",
        "    clf_results = []\n",
        "    \n",
        "    for each_frac in fractions:\n",
        "        max_queries = int(data_size*each_frac/100)\n",
        "        print(\"Attacking the victim with %d percent of training data: %d queries...\"%(each_frac, max_queries))\n",
        "#         partial_x_train, partial_y_train = subset_data(x_train, y_train, fraction=each_frac)\n",
        "        \n",
        "        sub_clf = build_substitute_model()\n",
        "        stolen_clf = KerasClassifier(model=sub_clf, clip_values=(0, 1), use_logits=True)\n",
        "        \n",
        "        attack = ART_attack(classifier=cloud_art_clf, \n",
        "                        batch_size_fit=32, \n",
        "                        batch_size_query=32,\n",
        "                        nb_epochs=epochs,\n",
        "                        nb_stolen=max_queries)\n",
        "        \n",
        "        stolen_clf = attack.extract(x_train, thieved_classifier=stolen_clf)\n",
        "\n",
        "        loss3, acc3 = stolen_clf._model.evaluate(x_train_adv, y_train_adv, verbose=verbose)\n",
        "        loss4, acc4 = stolen_clf._model.evaluate(x_test_adv, y_test_adv, verbose=verbose)\n",
        "        \n",
        "        clf_results.append((ART_attack.__name__, each_frac, max_queries, epochs,\n",
        "                            (round(loss1,3), acc1), (round(loss2,3), acc2), \n",
        "                            (round(loss3,3), acc3), (round(loss4,3), acc4)))\n",
        "        \n",
        "        results_df = pd.DataFrame(clf_results, columns=['Attack', 'Amount of data', 'Epochs','Queries needed', \n",
        "                                                         'Victim results on Train data',\n",
        "                                                       'Victim results on Test data', 'Clone model results on Train data', \n",
        "                                                        'Clone model results on Test data'])\n",
        "    return results_df"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSkB8I8dJOp9",
        "outputId": "eb4268a0-1c64-4fe0-d2fd-85365e459ce5"
      },
      "source": [
        "import pandas as pd\n",
        "KON_results_20e = extraction_attack(KnockoffNets, x_train, y_train, [5,10,30], epoch=30)\n",
        "KON_results_30e = extraction_attack(KnockoffNets, x_train, y_train, [5,10,30], epoch=10)\n",
        "KON_results_40e = extraction_attack(KnockoffNets, x_train, y_train, [5,10,30], epoch=35)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extract 100 Training samples from each class:\n",
            "(1000, 32, 32, 3) (1000, 10)\n",
            "Extract 100 Test samples from each class:\n",
            "(1000, 32, 32, 3) (1000, 10)\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2057: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attacking the victim with 5 percent of training data: 2500 queries...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attacking the victim with 10 percent of training data: 5000 queries...\n",
            "Attacking the victim with 30 percent of training data: 15000 queries...\n",
            "Extract 100 Training samples from each class:\n",
            "(1000, 32, 32, 3) (1000, 10)\n",
            "Extract 100 Test samples from each class:\n",
            "(1000, 32, 32, 3) (1000, 10)\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attacking the victim with 5 percent of training data: 2500 queries...\n",
            "Attacking the victim with 10 percent of training data: 5000 queries...\n",
            "Attacking the victim with 30 percent of training data: 15000 queries...\n",
            "Extract 100 Training samples from each class:\n",
            "(1000, 32, 32, 3) (1000, 10)\n",
            "Extract 100 Test samples from each class:\n",
            "(1000, 32, 32, 3) (1000, 10)\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attacking the victim with 5 percent of training data: 2500 queries...\n",
            "Attacking the victim with 10 percent of training data: 5000 queries...\n",
            "Attacking the victim with 30 percent of training data: 15000 queries...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRDceNJMTzbt"
      },
      "source": [
        "CCC_results_30e = extraction_attack(CopycatCNN, x_train, y_train, [5,10,30], epoch=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "6W9cktuUNRoD",
        "outputId": "f5ed8228-4453-48cc-8430-63d6728b6819"
      },
      "source": [
        "EA_results = pd.concat([KON_results_20e,KON_results_30e,KON_results_40e])\n",
        "EA_results.head(20)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Attack</th>\n",
              "      <th>Amount of data</th>\n",
              "      <th>Epochs</th>\n",
              "      <th>Queries needed</th>\n",
              "      <th>Victim results on Train data</th>\n",
              "      <th>Victim results on Test data</th>\n",
              "      <th>Clone model results on Train data</th>\n",
              "      <th>Clone model results on Test data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KnockoffNets</td>\n",
              "      <td>5</td>\n",
              "      <td>2500</td>\n",
              "      <td>30</td>\n",
              "      <td>(170.88, 0.098)</td>\n",
              "      <td>(170.812, 0.101)</td>\n",
              "      <td>(19.704, 0.102)</td>\n",
              "      <td>(18.824, 0.103)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KnockoffNets</td>\n",
              "      <td>10</td>\n",
              "      <td>5000</td>\n",
              "      <td>30</td>\n",
              "      <td>(170.88, 0.098)</td>\n",
              "      <td>(170.812, 0.101)</td>\n",
              "      <td>(25.059, 0.1)</td>\n",
              "      <td>(23.716, 0.099)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>KnockoffNets</td>\n",
              "      <td>30</td>\n",
              "      <td>15000</td>\n",
              "      <td>30</td>\n",
              "      <td>(170.88, 0.098)</td>\n",
              "      <td>(170.812, 0.101)</td>\n",
              "      <td>(27.243, 0.101)</td>\n",
              "      <td>(25.768, 0.101)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KnockoffNets</td>\n",
              "      <td>5</td>\n",
              "      <td>2500</td>\n",
              "      <td>10</td>\n",
              "      <td>(170.88, 0.098)</td>\n",
              "      <td>(170.812, 0.101)</td>\n",
              "      <td>(12.253, 0.1)</td>\n",
              "      <td>(12.218, 0.1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KnockoffNets</td>\n",
              "      <td>10</td>\n",
              "      <td>5000</td>\n",
              "      <td>10</td>\n",
              "      <td>(170.88, 0.098)</td>\n",
              "      <td>(170.812, 0.101)</td>\n",
              "      <td>(10.918, 0.101)</td>\n",
              "      <td>(10.651, 0.102)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>KnockoffNets</td>\n",
              "      <td>30</td>\n",
              "      <td>15000</td>\n",
              "      <td>10</td>\n",
              "      <td>(170.88, 0.098)</td>\n",
              "      <td>(170.812, 0.101)</td>\n",
              "      <td>(16.862, 0.1)</td>\n",
              "      <td>(16.012, 0.1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KnockoffNets</td>\n",
              "      <td>5</td>\n",
              "      <td>2500</td>\n",
              "      <td>35</td>\n",
              "      <td>(170.88, 0.098)</td>\n",
              "      <td>(170.812, 0.101)</td>\n",
              "      <td>(18.995, 0.096)</td>\n",
              "      <td>(18.983, 0.098)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KnockoffNets</td>\n",
              "      <td>10</td>\n",
              "      <td>5000</td>\n",
              "      <td>35</td>\n",
              "      <td>(170.88, 0.098)</td>\n",
              "      <td>(170.812, 0.101)</td>\n",
              "      <td>(19.094, 0.102)</td>\n",
              "      <td>(18.159, 0.101)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>KnockoffNets</td>\n",
              "      <td>30</td>\n",
              "      <td>15000</td>\n",
              "      <td>35</td>\n",
              "      <td>(170.88, 0.098)</td>\n",
              "      <td>(170.812, 0.101)</td>\n",
              "      <td>(26.524, 0.099)</td>\n",
              "      <td>(25.221, 0.099)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Attack  ...  Clone model results on Test data\n",
              "0  KnockoffNets  ...                   (18.824, 0.103)\n",
              "1  KnockoffNets  ...                   (23.716, 0.099)\n",
              "2  KnockoffNets  ...                   (25.768, 0.101)\n",
              "0  KnockoffNets  ...                     (12.218, 0.1)\n",
              "1  KnockoffNets  ...                   (10.651, 0.102)\n",
              "2  KnockoffNets  ...                     (16.012, 0.1)\n",
              "0  KnockoffNets  ...                   (18.983, 0.098)\n",
              "1  KnockoffNets  ...                   (18.159, 0.101)\n",
              "2  KnockoffNets  ...                   (25.221, 0.099)\n",
              "\n",
              "[9 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pin4t6aiTu35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "outputId": "33abd397-0af3-41ca-f231-39fbfcb84d06"
      },
      "source": [
        "EA_results = pd.concat([CCC_results_30e])\n",
        "EA_results.head(10)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Attack</th>\n",
              "      <th>Amount of data</th>\n",
              "      <th>Epochs</th>\n",
              "      <th>Queries needed</th>\n",
              "      <th>Victim results on Train data</th>\n",
              "      <th>Victim results on Test data</th>\n",
              "      <th>Clone model results on Train data</th>\n",
              "      <th>Clone model results on Test data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CopycatCNN</td>\n",
              "      <td>5</td>\n",
              "      <td>2500</td>\n",
              "      <td>30</td>\n",
              "      <td>(170.88, 0.098)</td>\n",
              "      <td>(170.812, 0.101)</td>\n",
              "      <td>(21.248, 0.102)</td>\n",
              "      <td>(19.783, 0.098)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CopycatCNN</td>\n",
              "      <td>10</td>\n",
              "      <td>5000</td>\n",
              "      <td>30</td>\n",
              "      <td>(170.88, 0.098)</td>\n",
              "      <td>(170.812, 0.101)</td>\n",
              "      <td>(24.865, 0.098)</td>\n",
              "      <td>(23.475, 0.1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CopycatCNN</td>\n",
              "      <td>30</td>\n",
              "      <td>15000</td>\n",
              "      <td>30</td>\n",
              "      <td>(170.88, 0.098)</td>\n",
              "      <td>(170.812, 0.101)</td>\n",
              "      <td>(27.923, 0.099)</td>\n",
              "      <td>(26.533, 0.1)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Attack  ...  Clone model results on Test data\n",
              "0  CopycatCNN  ...                   (19.783, 0.098)\n",
              "1  CopycatCNN  ...                     (23.475, 0.1)\n",
              "2  CopycatCNN  ...                     (26.533, 0.1)\n",
              "\n",
              "[3 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6Qs01Axad_G"
      },
      "source": [
        "def CCC_extraction_attack(npd_data, x_train, y_train, fractions, epoch=30, verbose=0):\n",
        "    \n",
        "    data_size = x_train.shape[0]\n",
        "    epochs = epoch\n",
        "    npd_size = npd_data.shape[0]\n",
        "    \n",
        "    ### Train and Test sample data from each class to get final results\n",
        "    print(\"Extract 100 Training samples from each class:\")\n",
        "    x_train_adv, y_train_adv = sample_by_class(x_train, y_train, num_samples=100)\n",
        "    print(\"Extract 100 Test samples from each class:\")\n",
        "    x_test_adv, y_test_adv = sample_by_class(x_test, y_test, num_samples=100)\n",
        "    \n",
        "    ge_cifar_clf = build_ganeval_model()\n",
        "    cloud_art_clf = KerasClassifier(model=ge_cifar_clf, clip_values=(0, 1), use_logits=True)\n",
        "    \n",
        "    loss1, acc1 = cloud_art_clf._model.evaluate(x_train_adv, y_train_adv, verbose=verbose)\n",
        "    loss2, acc2 = cloud_art_clf._model.evaluate(x_test_adv, y_test_adv, verbose=verbose)\n",
        "            \n",
        "    sub_clf = build_substitute_model()\n",
        "    stolen_clf = KerasClassifier(model=sub_clf, clip_values=(0, 1), use_logits=True)\n",
        "    \n",
        "    attack = CopycatCNN(classifier=cloud_art_clf, \n",
        "                        batch_size_fit=32, \n",
        "                        batch_size_query=32,\n",
        "                        nb_epochs=epochs,\n",
        "                        nb_stolen=npd_size)\n",
        "    \n",
        "    stolen_clf = attack.extract(npd_data, thieved_classifier=stolen_clf)\n",
        "    \n",
        "    stolen_clf._model.save_weights(\"CCC_temp.h5\")\n",
        "    \n",
        "    clf_results = []\n",
        "    \n",
        "    for each_frac in fractions:\n",
        "\n",
        "        max_queries = int(data_size*each_frac/100)\n",
        "        print(\"Attacking the victim with %d percent of training data: %d queries...\"%(each_frac, max_queries))\n",
        "        \n",
        "        stolen_clf._model.load_weights(\"CCC_temp.h5\")\n",
        "        partial_x_train, _ = subset_data(x_train, y_train, fraction=each_frac)\n",
        "\n",
        "        stolen_clf = attack.extract(partial_x_train, thieved_classifier=stolen_clf)\n",
        "        \n",
        "        loss3, acc3 = stolen_clf._model.evaluate(x_train_adv, y_train_adv, verbose=verbose)\n",
        "        loss4, acc4 = stolen_clf._model.evaluate(x_test_adv, y_test_adv, verbose=verbose)\n",
        "        \n",
        "        clf_results.append((\"Copycat CNN\", npd_size, each_frac, max_queries, epochs, \n",
        "                            (round(loss1,3), acc1), \n",
        "                            (round(loss2,3), acc2), \n",
        "                            (round(loss3,3), acc3), \n",
        "                            (round(loss4,3), acc4)))\n",
        "        \n",
        "        results_df = pd.DataFrame(clf_results, columns=['Attack', \"NPD Queries\", 'Fraction of train data', \n",
        "                                                        'Train queries', \n",
        "                                                        'Epochs trained', 'Victim clf on Train data',\n",
        "                                                       'Victim clf on Test data', 'Stolen clf on Train data', \n",
        "                                                        'Stolen clf on Test data'])\n",
        "    return results_df"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXWmYMaMaeKS",
        "outputId": "4fcd5e18-6ac3-479b-e9da-33423165da5a"
      },
      "source": [
        "from tensorflow.keras.datasets import cifar100\n",
        "(cifar100_x, cifar100_y), _ = cifar100.load_data(label_mode='fine')\n",
        "print('cifar100 training data shape:', cifar100_x.shape)\n",
        "\n",
        "# Normalize pixel values\n",
        "cifar100_x = cifar100_x/255\n",
        "\n",
        "# Sample 20,000 data points randomly from the data\n",
        "idx = np.random.choice(cifar100_x.shape[0], 20000, replace=False)\n",
        "cifar100_x = cifar100_x[idx].copy()\n",
        "cifar100_y = cifar100_y[idx].copy()\n",
        "\n",
        "print('Final NPD size of cifar100 training data:', cifar100_x.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 3s 0us/step\n",
            "169017344/169001437 [==============================] - 3s 0us/step\n",
            "cifar100 training data shape: (50000, 32, 32, 3)\n",
            "Final NPD size of cifar100 training data: (20000, 32, 32, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-D09-vnakIK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58e25843-0002-4932-b8a8-67814a059caa"
      },
      "source": [
        "CCC_results_30e = CCC_extraction_attack(cifar100_x, x_train, y_train, [5,10,30], epoch=30)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extract 100 Training samples from each class:\n",
            "(1000, 32, 32, 3) (1000, 10)\n",
            "Extract 100 Test samples from each class:\n",
            "(1000, 32, 32, 3) (1000, 10)\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2057: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 20000 samples\n",
            "Epoch 1/30\n",
            "20000/20000 [==============================] - 9s 457us/sample - loss: 2.0761 - accuracy: 0.2313\n",
            "Epoch 2/30\n",
            "20000/20000 [==============================] - 9s 448us/sample - loss: 1.9525 - accuracy: 0.2813\n",
            "Epoch 3/30\n",
            "20000/20000 [==============================] - 9s 462us/sample - loss: 1.8846 - accuracy: 0.3091\n",
            "Epoch 4/30\n",
            "20000/20000 [==============================] - 12s 593us/sample - loss: 1.8144 - accuracy: 0.3335\n",
            "Epoch 5/30\n",
            "20000/20000 [==============================] - 12s 602us/sample - loss: 1.7576 - accuracy: 0.3537\n",
            "Epoch 6/30\n",
            "20000/20000 [==============================] - 12s 589us/sample - loss: 1.7045 - accuracy: 0.3679\n",
            "Epoch 7/30\n",
            "20000/20000 [==============================] - 12s 575us/sample - loss: 1.6526 - accuracy: 0.3895\n",
            "Epoch 8/30\n",
            "20000/20000 [==============================] - 11s 572us/sample - loss: 1.6095 - accuracy: 0.4099\n",
            "Epoch 9/30\n",
            "20000/20000 [==============================] - 12s 576us/sample - loss: 1.5640 - accuracy: 0.4232\n",
            "Epoch 10/30\n",
            "20000/20000 [==============================] - 11s 565us/sample - loss: 1.5246 - accuracy: 0.4378\n",
            "Epoch 11/30\n",
            "20000/20000 [==============================] - 11s 565us/sample - loss: 1.4779 - accuracy: 0.4534\n",
            "Epoch 12/30\n",
            "20000/20000 [==============================] - 11s 554us/sample - loss: 1.4317 - accuracy: 0.4717\n",
            "Epoch 13/30\n",
            "20000/20000 [==============================] - 11s 550us/sample - loss: 1.3811 - accuracy: 0.4891\n",
            "Epoch 14/30\n",
            "20000/20000 [==============================] - 11s 553us/sample - loss: 1.3494 - accuracy: 0.5003\n",
            "Epoch 15/30\n",
            "20000/20000 [==============================] - 11s 548us/sample - loss: 1.3026 - accuracy: 0.5185\n",
            "Epoch 16/30\n",
            "20000/20000 [==============================] - 11s 544us/sample - loss: 1.2615 - accuracy: 0.5383\n",
            "Epoch 17/30\n",
            "20000/20000 [==============================] - 11s 541us/sample - loss: 1.2317 - accuracy: 0.5457\n",
            "Epoch 18/30\n",
            "20000/20000 [==============================] - 10s 522us/sample - loss: 1.1768 - accuracy: 0.5681\n",
            "Epoch 19/30\n",
            "20000/20000 [==============================] - 10s 518us/sample - loss: 1.1620 - accuracy: 0.5742\n",
            "Epoch 20/30\n",
            "20000/20000 [==============================] - 10s 499us/sample - loss: 1.1269 - accuracy: 0.5918\n",
            "Epoch 21/30\n",
            "20000/20000 [==============================] - 10s 501us/sample - loss: 1.1005 - accuracy: 0.5975\n",
            "Epoch 22/30\n",
            "20000/20000 [==============================] - 10s 496us/sample - loss: 1.0689 - accuracy: 0.6057\n",
            "Epoch 23/30\n",
            "20000/20000 [==============================] - 10s 495us/sample - loss: 1.0334 - accuracy: 0.6198\n",
            "Epoch 24/30\n",
            "20000/20000 [==============================] - 10s 483us/sample - loss: 1.0047 - accuracy: 0.6284\n",
            "Epoch 25/30\n",
            "20000/20000 [==============================] - 10s 478us/sample - loss: 0.9851 - accuracy: 0.6404\n",
            "Epoch 26/30\n",
            "20000/20000 [==============================] - 10s 488us/sample - loss: 0.9585 - accuracy: 0.6520\n",
            "Epoch 27/30\n",
            "20000/20000 [==============================] - 10s 481us/sample - loss: 0.9395 - accuracy: 0.6594\n",
            "Epoch 28/30\n",
            "20000/20000 [==============================] - 10s 478us/sample - loss: 0.9282 - accuracy: 0.6673\n",
            "Epoch 29/30\n",
            "20000/20000 [==============================] - 10s 476us/sample - loss: 0.8967 - accuracy: 0.6734\n",
            "Epoch 30/30\n",
            "20000/20000 [==============================] - 9s 469us/sample - loss: 0.8908 - accuracy: 0.6764\n",
            "Attacking the victim with 5 percent of training data: 2500 queries...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:art.attacks.extraction.copycat_cnn:The size of the source input is smaller than the expected number of queries submitted to the victim classifier.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2500, 32, 32, 3) (2500, 10)\n",
            "Train on 2500 samples\n",
            "Epoch 1/30\n",
            "2500/2500 [==============================] - 1s 436us/sample - loss: 31.2618 - accuracy: 0.8280\n",
            "Epoch 2/30\n",
            "2500/2500 [==============================] - 1s 412us/sample - loss: 1.8237 - accuracy: 0.8544\n",
            "Epoch 3/30\n",
            "2500/2500 [==============================] - 1s 423us/sample - loss: 0.8788 - accuracy: 0.8780\n",
            "Epoch 4/30\n",
            "2500/2500 [==============================] - 1s 427us/sample - loss: 0.6731 - accuracy: 0.8992\n",
            "Epoch 5/30\n",
            "2500/2500 [==============================] - 1s 447us/sample - loss: 0.5644 - accuracy: 0.9156\n",
            "Epoch 6/30\n",
            "2500/2500 [==============================] - 1s 446us/sample - loss: 0.4417 - accuracy: 0.9200\n",
            "Epoch 7/30\n",
            "2500/2500 [==============================] - 1s 453us/sample - loss: 0.3591 - accuracy: 0.9236\n",
            "Epoch 8/30\n",
            "2500/2500 [==============================] - 1s 448us/sample - loss: 0.3598 - accuracy: 0.9236\n",
            "Epoch 9/30\n",
            "2500/2500 [==============================] - 1s 460us/sample - loss: 0.3672 - accuracy: 0.9228\n",
            "Epoch 10/30\n",
            "2500/2500 [==============================] - 1s 456us/sample - loss: 0.3527 - accuracy: 0.9216\n",
            "Epoch 11/30\n",
            "2500/2500 [==============================] - 1s 457us/sample - loss: 0.3707 - accuracy: 0.9220\n",
            "Epoch 12/30\n",
            "2500/2500 [==============================] - 1s 442us/sample - loss: 0.3486 - accuracy: 0.9216\n",
            "Epoch 13/30\n",
            "2500/2500 [==============================] - 1s 455us/sample - loss: 0.3399 - accuracy: 0.9252\n",
            "Epoch 14/30\n",
            "2500/2500 [==============================] - 1s 472us/sample - loss: 0.3295 - accuracy: 0.9240\n",
            "Epoch 15/30\n",
            "2500/2500 [==============================] - 1s 471us/sample - loss: 0.3436 - accuracy: 0.9240\n",
            "Epoch 16/30\n",
            "2500/2500 [==============================] - 1s 457us/sample - loss: 0.3312 - accuracy: 0.9248\n",
            "Epoch 17/30\n",
            "2500/2500 [==============================] - 1s 462us/sample - loss: 0.3505 - accuracy: 0.9244\n",
            "Epoch 18/30\n",
            "2500/2500 [==============================] - 1s 466us/sample - loss: 0.3355 - accuracy: 0.9240\n",
            "Epoch 19/30\n",
            "2500/2500 [==============================] - 1s 474us/sample - loss: 0.3513 - accuracy: 0.9256\n",
            "Epoch 20/30\n",
            "2500/2500 [==============================] - 1s 490us/sample - loss: 0.3556 - accuracy: 0.9228\n",
            "Epoch 21/30\n",
            "2500/2500 [==============================] - 1s 484us/sample - loss: 0.3356 - accuracy: 0.9252\n",
            "Epoch 22/30\n",
            "2500/2500 [==============================] - 1s 466us/sample - loss: 0.3447 - accuracy: 0.9240\n",
            "Epoch 23/30\n",
            "2500/2500 [==============================] - 1s 471us/sample - loss: 0.3412 - accuracy: 0.9248\n",
            "Epoch 24/30\n",
            "2500/2500 [==============================] - 1s 473us/sample - loss: 0.3501 - accuracy: 0.9236\n",
            "Epoch 25/30\n",
            "2500/2500 [==============================] - 1s 467us/sample - loss: 0.3340 - accuracy: 0.9252\n",
            "Epoch 26/30\n",
            "2500/2500 [==============================] - 1s 469us/sample - loss: 0.3559 - accuracy: 0.9232\n",
            "Epoch 27/30\n",
            "2500/2500 [==============================] - 1s 472us/sample - loss: 0.3384 - accuracy: 0.9236\n",
            "Epoch 28/30\n",
            "2500/2500 [==============================] - 1s 470us/sample - loss: 0.3299 - accuracy: 0.9244\n",
            "Epoch 29/30\n",
            "2500/2500 [==============================] - 1s 473us/sample - loss: 0.3360 - accuracy: 0.9248\n",
            "Epoch 30/30\n",
            "2500/2500 [==============================] - 1s 474us/sample - loss: 0.3274 - accuracy: 0.9256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:art.attacks.extraction.copycat_cnn:The size of the source input is smaller than the expected number of queries submitted to the victim classifier.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attacking the victim with 10 percent of training data: 5000 queries...\n",
            "(5000, 32, 32, 3) (5000, 10)\n",
            "Train on 5000 samples\n",
            "Epoch 1/30\n",
            "5000/5000 [==============================] - 2s 439us/sample - loss: 19.5105 - accuracy: 0.8404\n",
            "Epoch 2/30\n",
            "5000/5000 [==============================] - 2s 433us/sample - loss: 0.8131 - accuracy: 0.8764\n",
            "Epoch 3/30\n",
            "5000/5000 [==============================] - 2s 434us/sample - loss: 0.6375 - accuracy: 0.9068\n",
            "Epoch 4/30\n",
            "5000/5000 [==============================] - 2s 440us/sample - loss: 0.5075 - accuracy: 0.9200\n",
            "Epoch 5/30\n",
            "5000/5000 [==============================] - 2s 423us/sample - loss: 0.4587 - accuracy: 0.9246\n",
            "Epoch 6/30\n",
            "5000/5000 [==============================] - 2s 439us/sample - loss: 0.4283 - accuracy: 0.9264\n",
            "Epoch 7/30\n",
            "5000/5000 [==============================] - 2s 445us/sample - loss: 0.3912 - accuracy: 0.9266\n",
            "Epoch 8/30\n",
            "5000/5000 [==============================] - 2s 445us/sample - loss: 0.3690 - accuracy: 0.9268\n",
            "Epoch 9/30\n",
            "5000/5000 [==============================] - 2s 429us/sample - loss: 0.3806 - accuracy: 0.9262\n",
            "Epoch 10/30\n",
            "5000/5000 [==============================] - 2s 421us/sample - loss: 0.3727 - accuracy: 0.9268\n",
            "Epoch 11/30\n",
            "5000/5000 [==============================] - 2s 426us/sample - loss: 0.3729 - accuracy: 0.9258\n",
            "Epoch 12/30\n",
            "5000/5000 [==============================] - 2s 444us/sample - loss: 0.3622 - accuracy: 0.9268\n",
            "Epoch 13/30\n",
            "5000/5000 [==============================] - 2s 451us/sample - loss: 0.3491 - accuracy: 0.9268\n",
            "Epoch 14/30\n",
            "5000/5000 [==============================] - 2s 454us/sample - loss: 0.3344 - accuracy: 0.9270\n",
            "Epoch 15/30\n",
            "5000/5000 [==============================] - 2s 451us/sample - loss: 0.3354 - accuracy: 0.9268\n",
            "Epoch 16/30\n",
            "5000/5000 [==============================] - 2s 449us/sample - loss: 0.3380 - accuracy: 0.9270\n",
            "Epoch 17/30\n",
            "5000/5000 [==============================] - 2s 456us/sample - loss: 0.3287 - accuracy: 0.9272\n",
            "Epoch 18/30\n",
            "5000/5000 [==============================] - 2s 457us/sample - loss: 0.3316 - accuracy: 0.9270\n",
            "Epoch 19/30\n",
            "5000/5000 [==============================] - 2s 457us/sample - loss: 0.3160 - accuracy: 0.9272\n",
            "Epoch 20/30\n",
            "5000/5000 [==============================] - 2s 462us/sample - loss: 0.3172 - accuracy: 0.9272\n",
            "Epoch 21/30\n",
            "5000/5000 [==============================] - 2s 462us/sample - loss: 0.3184 - accuracy: 0.9272\n",
            "Epoch 22/30\n",
            "5000/5000 [==============================] - 2s 458us/sample - loss: 0.3173 - accuracy: 0.9276\n",
            "Epoch 23/30\n",
            "5000/5000 [==============================] - 2s 450us/sample - loss: 0.3109 - accuracy: 0.9270\n",
            "Epoch 24/30\n",
            "5000/5000 [==============================] - 2s 455us/sample - loss: 0.3171 - accuracy: 0.9274\n",
            "Epoch 25/30\n",
            "5000/5000 [==============================] - 2s 473us/sample - loss: 0.3144 - accuracy: 0.9272\n",
            "Epoch 26/30\n",
            "5000/5000 [==============================] - 2s 451us/sample - loss: 0.3058 - accuracy: 0.9278\n",
            "Epoch 27/30\n",
            "5000/5000 [==============================] - 2s 461us/sample - loss: 0.3192 - accuracy: 0.9274\n",
            "Epoch 28/30\n",
            "5000/5000 [==============================] - 2s 452us/sample - loss: 0.3134 - accuracy: 0.9268\n",
            "Epoch 29/30\n",
            "5000/5000 [==============================] - 2s 478us/sample - loss: 0.3093 - accuracy: 0.9270\n",
            "Epoch 30/30\n",
            "5000/5000 [==============================] - 2s 452us/sample - loss: 0.3235 - accuracy: 0.9270\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:art.attacks.extraction.copycat_cnn:The size of the source input is smaller than the expected number of queries submitted to the victim classifier.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attacking the victim with 30 percent of training data: 15000 queries...\n",
            "(15000, 32, 32, 3) (15000, 10)\n",
            "Train on 15000 samples\n",
            "Epoch 1/30\n",
            "15000/15000 [==============================] - 7s 441us/sample - loss: 6.0331 - accuracy: 0.8691\n",
            "Epoch 2/30\n",
            "15000/15000 [==============================] - 7s 461us/sample - loss: 0.5243 - accuracy: 0.9253\n",
            "Epoch 3/30\n",
            "15000/15000 [==============================] - 7s 474us/sample - loss: 0.4176 - accuracy: 0.9253\n",
            "Epoch 4/30\n",
            "15000/15000 [==============================] - 7s 471us/sample - loss: 0.3856 - accuracy: 0.9254\n",
            "Epoch 5/30\n",
            "15000/15000 [==============================] - 7s 462us/sample - loss: 0.3708 - accuracy: 0.9259\n",
            "Epoch 6/30\n",
            "15000/15000 [==============================] - 7s 473us/sample - loss: 0.3532 - accuracy: 0.9257\n",
            "Epoch 7/30\n",
            "15000/15000 [==============================] - 7s 467us/sample - loss: 0.3441 - accuracy: 0.9257\n",
            "Epoch 8/30\n",
            "15000/15000 [==============================] - 7s 471us/sample - loss: 0.3463 - accuracy: 0.9256\n",
            "Epoch 9/30\n",
            "15000/15000 [==============================] - 7s 463us/sample - loss: 0.3316 - accuracy: 0.9259\n",
            "Epoch 10/30\n",
            "15000/15000 [==============================] - 7s 463us/sample - loss: 0.3318 - accuracy: 0.9259\n",
            "Epoch 11/30\n",
            "15000/15000 [==============================] - 7s 466us/sample - loss: 0.3265 - accuracy: 0.9258\n",
            "Epoch 12/30\n",
            "15000/15000 [==============================] - 7s 466us/sample - loss: 0.3279 - accuracy: 0.9259\n",
            "Epoch 13/30\n",
            "15000/15000 [==============================] - 7s 463us/sample - loss: 0.3203 - accuracy: 0.9259\n",
            "Epoch 14/30\n",
            "15000/15000 [==============================] - 7s 469us/sample - loss: 0.3203 - accuracy: 0.9260\n",
            "Epoch 15/30\n",
            "15000/15000 [==============================] - 7s 470us/sample - loss: 0.3278 - accuracy: 0.9255\n",
            "Epoch 16/30\n",
            "15000/15000 [==============================] - 7s 464us/sample - loss: 0.3337 - accuracy: 0.9251\n",
            "Epoch 17/30\n",
            "15000/15000 [==============================] - 7s 469us/sample - loss: 0.3453 - accuracy: 0.9252\n",
            "Epoch 18/30\n",
            "15000/15000 [==============================] - 7s 471us/sample - loss: 0.3224 - accuracy: 0.9259\n",
            "Epoch 19/30\n",
            "15000/15000 [==============================] - 7s 460us/sample - loss: 0.3207 - accuracy: 0.9259\n",
            "Epoch 20/30\n",
            "15000/15000 [==============================] - 7s 471us/sample - loss: 0.3203 - accuracy: 0.9259\n",
            "Epoch 21/30\n",
            "15000/15000 [==============================] - 7s 465us/sample - loss: 0.3204 - accuracy: 0.9259\n",
            "Epoch 22/30\n",
            "15000/15000 [==============================] - 7s 471us/sample - loss: 0.3289 - accuracy: 0.9259\n",
            "Epoch 23/30\n",
            "15000/15000 [==============================] - 7s 483us/sample - loss: 0.3179 - accuracy: 0.9259\n",
            "Epoch 24/30\n",
            "15000/15000 [==============================] - 7s 464us/sample - loss: 0.3233 - accuracy: 0.9259\n",
            "Epoch 25/30\n",
            "15000/15000 [==============================] - 7s 472us/sample - loss: 0.3173 - accuracy: 0.9259\n",
            "Epoch 26/30\n",
            "15000/15000 [==============================] - 7s 470us/sample - loss: 0.3172 - accuracy: 0.9259\n",
            "Epoch 27/30\n",
            "15000/15000 [==============================] - 7s 467us/sample - loss: 0.3170 - accuracy: 0.9259\n",
            "Epoch 28/30\n",
            "15000/15000 [==============================] - 7s 466us/sample - loss: 0.3169 - accuracy: 0.9259\n",
            "Epoch 29/30\n",
            "15000/15000 [==============================] - 7s 468us/sample - loss: 0.3168 - accuracy: 0.9259\n",
            "Epoch 30/30\n",
            "15000/15000 [==============================] - 7s 468us/sample - loss: 0.3170 - accuracy: 0.9259\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx51LrbVakS3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "26c84450-75eb-4cb8-f16e-b3a2660a9489"
      },
      "source": [
        "CCC_results = pd.concat([CCC_results_30e])\n",
        "CCC_results.head(24)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Attack</th>\n",
              "      <th>NPD Queries</th>\n",
              "      <th>Fraction of train data</th>\n",
              "      <th>Train queries</th>\n",
              "      <th>Epochs trained</th>\n",
              "      <th>Victim clf on Train data</th>\n",
              "      <th>Victim clf on Test data</th>\n",
              "      <th>Stolen clf on Train data</th>\n",
              "      <th>Stolen clf on Test data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Copycat CNN</td>\n",
              "      <td>20000</td>\n",
              "      <td>5</td>\n",
              "      <td>2500</td>\n",
              "      <td>30</td>\n",
              "      <td>(170.88, 0.098)</td>\n",
              "      <td>(170.812, 0.101)</td>\n",
              "      <td>(8.612, 0.1)</td>\n",
              "      <td>(8.445, 0.1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Copycat CNN</td>\n",
              "      <td>20000</td>\n",
              "      <td>10</td>\n",
              "      <td>5000</td>\n",
              "      <td>30</td>\n",
              "      <td>(170.88, 0.098)</td>\n",
              "      <td>(170.812, 0.101)</td>\n",
              "      <td>(30.454, 0.1)</td>\n",
              "      <td>(30.825, 0.1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Copycat CNN</td>\n",
              "      <td>20000</td>\n",
              "      <td>30</td>\n",
              "      <td>15000</td>\n",
              "      <td>30</td>\n",
              "      <td>(170.88, 0.098)</td>\n",
              "      <td>(170.812, 0.101)</td>\n",
              "      <td>(6.978, 0.1)</td>\n",
              "      <td>(6.978, 0.1)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Attack  NPD Queries  ...  Stolen clf on Train data  Stolen clf on Test data\n",
              "0  Copycat CNN        20000  ...              (8.612, 0.1)             (8.445, 0.1)\n",
              "1  Copycat CNN        20000  ...             (30.454, 0.1)            (30.825, 0.1)\n",
              "2  Copycat CNN        20000  ...              (6.978, 0.1)             (6.978, 0.1)\n",
              "\n",
              "[3 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Amq0tsJEjlHW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7365257f-e6ef-45b2-bd10-5f21da77d7e7"
      },
      "source": [
        "x_train_adv, y_train_adv = sample_by_class(x_train, y_train, num_samples=100)\n",
        "x_test_adv, y_test_adv = sample_by_class(x_test, y_test, num_samples=100)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 32, 32, 3) (1000, 10)\n",
            "(1000, 32, 32, 3) (1000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H74-tZIFjlS6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd11b785-de6b-49ae-9fdb-f5f603efb9d4"
      },
      "source": [
        "partial_x_train, partial_y_train = subset_data(x_train, y_train, fraction=0.3)\n",
        "\n",
        "partial_x_train = partial_x_train.reshape(-1,32*32*3)\n",
        "\n",
        "### Tried even with standardizing the values around '0'\n",
        "x_mean = partial_x_train.mean()\n",
        "x_std = partial_x_train.std()\n",
        "# partial_x_train = (partial_x_train - x_mean)/x_std\n",
        "\n",
        "partial_x_train.shape"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(150, 32, 32, 3) (150, 10)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150, 3072)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uR1H73Ej0Pn"
      },
      "source": [
        "from keras.models import Sequential\n",
        "fen_eval_model = Sequential()\n",
        "fen_eval_model.add(Dense(1024, activation=\"relu\", input_shape=(32*32*3,)))\n",
        "fen_eval_model.add(Dense(10, activation=\"linear\"))\n",
        "\n",
        "fen_eval_model.compile(\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001,),\n",
        "    metrics=[\"accuracy\"],\n",
        ")"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCX5LSpsj0Yy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "626b283f-da6c-43bb-a3d2-0de658a0eabf"
      },
      "source": [
        "fen_eval_model.fit(x_train.reshape(-1,32*32*3), y_train, batch_size=32, epochs=30, verbose=1)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 50000 samples\n",
            "Epoch 1/30\n",
            "50000/50000 [==============================] - 8s 167us/sample - loss: 30.0469 - accuracy: 0.2512\n",
            "Epoch 2/30\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 13.6426 - accuracy: 0.2796\n",
            "Epoch 3/30\n",
            "50000/50000 [==============================] - 8s 163us/sample - loss: 9.0106 - accuracy: 0.2999\n",
            "Epoch 4/30\n",
            "50000/50000 [==============================] - 8s 163us/sample - loss: 6.0566 - accuracy: 0.3033\n",
            "Epoch 5/30\n",
            "50000/50000 [==============================] - 8s 164us/sample - loss: 3.0482 - accuracy: 0.2352\n",
            "Epoch 6/30\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 2.0673 - accuracy: 0.2635\n",
            "Epoch 7/30\n",
            "50000/50000 [==============================] - 8s 163us/sample - loss: 1.9065 - accuracy: 0.2971\n",
            "Epoch 8/30\n",
            "50000/50000 [==============================] - 8s 161us/sample - loss: 1.8446 - accuracy: 0.3118\n",
            "Epoch 9/30\n",
            "50000/50000 [==============================] - 8s 163us/sample - loss: 1.8196 - accuracy: 0.3164\n",
            "Epoch 10/30\n",
            "50000/50000 [==============================] - 8s 163us/sample - loss: 1.7973 - accuracy: 0.3263\n",
            "Epoch 11/30\n",
            "50000/50000 [==============================] - 8s 161us/sample - loss: 1.7789 - accuracy: 0.3375\n",
            "Epoch 12/30\n",
            "50000/50000 [==============================] - 8s 166us/sample - loss: 1.7688 - accuracy: 0.3402\n",
            "Epoch 13/30\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 1.7586 - accuracy: 0.3503\n",
            "Epoch 14/30\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 1.7481 - accuracy: 0.3552\n",
            "Epoch 15/30\n",
            "50000/50000 [==============================] - 8s 162us/sample - loss: 1.7405 - accuracy: 0.3556\n",
            "Epoch 16/30\n",
            "50000/50000 [==============================] - 8s 163us/sample - loss: 1.7296 - accuracy: 0.3621\n",
            "Epoch 17/30\n",
            "50000/50000 [==============================] - 8s 164us/sample - loss: 1.7286 - accuracy: 0.3636\n",
            "Epoch 18/30\n",
            "50000/50000 [==============================] - 8s 166us/sample - loss: 1.7179 - accuracy: 0.3681\n",
            "Epoch 19/30\n",
            "50000/50000 [==============================] - 8s 164us/sample - loss: 1.7114 - accuracy: 0.3685\n",
            "Epoch 20/30\n",
            "50000/50000 [==============================] - 8s 163us/sample - loss: 1.7062 - accuracy: 0.3736\n",
            "Epoch 21/30\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 1.7047 - accuracy: 0.3762\n",
            "Epoch 22/30\n",
            "50000/50000 [==============================] - 8s 164us/sample - loss: 1.6995 - accuracy: 0.3773\n",
            "Epoch 23/30\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 1.6940 - accuracy: 0.3804\n",
            "Epoch 24/30\n",
            "50000/50000 [==============================] - 8s 166us/sample - loss: 1.6879 - accuracy: 0.3827\n",
            "Epoch 25/30\n",
            "50000/50000 [==============================] - 8s 166us/sample - loss: 1.6877 - accuracy: 0.3836\n",
            "Epoch 26/30\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 1.6834 - accuracy: 0.3861\n",
            "Epoch 27/30\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 1.6775 - accuracy: 0.3893\n",
            "Epoch 28/30\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 1.6724 - accuracy: 0.3898\n",
            "Epoch 29/30\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 1.6670 - accuracy: 0.3926\n",
            "Epoch 30/30\n",
            "50000/50000 [==============================] - 8s 167us/sample - loss: 1.6606 - accuracy: 0.3953\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc050437790>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-6r_I7tkKSI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5f7609a-e835-4db9-c9b1-6770879f419f"
      },
      "source": [
        "fen_eval_model.evaluate(x_test.reshape(-1,32*32*3), y_test)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2057: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.7292973426818847, 0.3781]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vDu9_8oj0gA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4933a6ce-ac27-47c2-9e93-c5d636bf1383"
      },
      "source": [
        "fen_eval_model.evaluate(partial_x_train, partial_y_train)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.5711732721328735, 0.42]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1zPHfUflW6G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        },
        "outputId": "c2a6d11e-a51a-4a3f-fdcd-9af327bd77af"
      },
      "source": [
        "flat_art_clf = KerasClassifier(model=fen_eval_model, clip_values=(0, 1), use_logits=False)\n",
        "\n",
        "attack_FEN = FunctionallyEquivalentExtraction(classifier=flat_art_clf, num_neurons=1024)\n",
        "stolen_clf_FEN = attack_FEN.extract(partial_x_train, partial_y_train,\n",
        "                                    delta_0=0.06,\n",
        "                                    fraction_true=0.3,\n",
        "                                    rel_diff_slope=0.1,\n",
        "                                    rel_diff_value=0.1,\n",
        "                                    delta_init_value=0.1,\n",
        "                                    delta_value_max=2,\n",
        "                                    d2_min=0.04,\n",
        "                                    d_step=0.01,\n",
        "                                    delta_sign=0.2,\n",
        "                                    unit_vector_scale=1000)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "/usr/local/lib/python3.7/dist-packages/art/attacks/extraction/functionally_equivalent_extraction.py:221: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  if np.sum(np.abs((m_1 - m_2) / m_1) < rel_diff_slope) > fraction_true * self.num_classes:\n",
            "/usr/local/lib/python3.7/dist-packages/art/attacks/extraction/functionally_equivalent_extraction.py:221: RuntimeWarning: invalid value encountered in true_divide\n",
            "  if np.sum(np.abs((m_1 - m_2) / m_1) < rel_diff_slope) > fraction_true * self.num_classes:\n",
            "/usr/local/lib/python3.7/dist-packages/art/attacks/extraction/functionally_equivalent_extraction.py:225: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  t_hat = t_1 + np.divide(y_2 - y_1 - (t_2 - t_1) * m_2, m_1 - m_2)\n",
            "/usr/local/lib/python3.7/dist-packages/art/attacks/extraction/functionally_equivalent_extraction.py:226: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  y_hat = y_1 + m_1 * np.divide(y_2 - y_1 - (t_2 - t_1) * m_2, m_1 - m_2)\n",
            "/usr/local/lib/python3.7/dist-packages/art/attacks/extraction/functionally_equivalent_extraction.py:226: RuntimeWarning: invalid value encountered in multiply\n",
            "  y_hat = y_1 + m_1 * np.divide(y_2 - y_1 - (t_2 - t_1) * m_2, m_1 - m_2)\n",
            "/usr/local/lib/python3.7/dist-packages/art/attacks/extraction/functionally_equivalent_extraction.py:242: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  and np.sum(np.abs((m_x_1 - m_x_2) / m_x_1) > rel_diff_slope) > fraction_true * self.num_classes\n",
            "/usr/local/lib/python3.7/dist-packages/art/attacks/extraction/functionally_equivalent_extraction.py:225: RuntimeWarning: invalid value encountered in true_divide\n",
            "  t_hat = t_1 + np.divide(y_2 - y_1 - (t_2 - t_1) * m_2, m_1 - m_2)\n",
            "/usr/local/lib/python3.7/dist-packages/art/attacks/extraction/functionally_equivalent_extraction.py:226: RuntimeWarning: invalid value encountered in true_divide\n",
            "  y_hat = y_1 + m_1 * np.divide(y_2 - y_1 - (t_2 - t_1) * m_2, m_1 - m_2)\n",
            "/usr/local/lib/python3.7/dist-packages/art/attacks/extraction/functionally_equivalent_extraction.py:215: RuntimeWarning: invalid value encountered in true_divide\n",
            "  m_1 = (self._o_l(x_1_p) - self._o_l(x_1)) / epsilon\n",
            "/usr/local/lib/python3.7/dist-packages/art/attacks/extraction/functionally_equivalent_extraction.py:216: RuntimeWarning: invalid value encountered in true_divide\n",
            "  m_2 = (self._o_l(x_2) - self._o_l(x_2_m)) / epsilon\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-2cf8a07518d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                                     \u001b[0md_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                     \u001b[0mdelta_sign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                                     unit_vector_scale=1000)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/art/attacks/extraction/functionally_equivalent_extraction.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(self, x, y, delta_0, fraction_true, rel_diff_slope, rel_diff_value, delta_init_value, delta_value_max, d2_min, d_step, delta_sign, unit_vector_scale, ftol, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mfraction_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfraction_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mrel_diff_slope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrel_diff_slope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0mrel_diff_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrel_diff_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         )\n\u001b[1;32m    125\u001b[0m         self._weight_recovery(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/art/attacks/extraction/functionally_equivalent_extraction.py\u001b[0m in \u001b[0;36m_critical_point_search\u001b[0;34m(self, delta_0, fraction_true, rel_diff_slope, rel_diff_value)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_o_l\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                 \u001b[0mm_x_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_o_l\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_mean_p\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_o_l\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m                 \u001b[0mm_x_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_o_l\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_mean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_o_l\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_mean_m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/art/attacks/extraction/functionally_equivalent_extraction.py\u001b[0m in \u001b[0;36m_o_l\u001b[0;34m(self, x, e_j)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0me_j\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0me_j\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUMPY_DTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_t\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/art/estimators/classification/classifier.py\u001b[0m in \u001b[0;36mreplacement_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfunc_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mreplacement_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfunc_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/art/estimators/classification/keras.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, training_mode, **kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_preprocessed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_preprocessed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;31m# Apply postprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    977\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m         callbacks=callbacks)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   4185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4186\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 4187\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   4188\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4189\u001b[0m     output_structure = tf.nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1483\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1484\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1486\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNI1OnO5lW9S"
      },
      "source": [
        "def build_ganeval_model_flat():\n",
        "    K.clear_session()\n",
        "    model_url = \"https://tfhub.dev/deepmind/ganeval-cifar10-convnet/1\"\n",
        "    ganeval_module = thub.Module(model_url)\n",
        "    \n",
        "    model = Sequential()\n",
        "#     model.add(InputLayer(input_shape=(32*32*3,)))\n",
        "    model.add(model.reshape((32,32,3),input_shape=(32*32*3,)))\n",
        "    model.add(thub.KerasLayer(ganeval_module))\n",
        "    model.add(Activation('relu'))\n",
        "        \n",
        "    model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.1, momentum=0.9, decay=1e-4),\n",
        "                  loss=CategoricalCrossentropy(),\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GsGRalLlXCX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "d858d785-e6c1-4cbf-9bd1-e99ab6437e7b"
      },
      "source": [
        "ge_flat_clf = build_ganeval_model_flat()\n",
        "ge_flat_clf.summary()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-200aeaaafe34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mge_flat_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_ganeval_model_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mge_flat_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-ae1c3e3d2cae>\u001b[0m in \u001b[0;36mbuild_ganeval_model_flat\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#     model.add(InputLayer(input_shape=(32*32*3,)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mganeval_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'reshape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLO4jEZSlokg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "716a3560-9645-427b-ccac-6d85a71db311"
      },
      "source": [
        "ge_flat_clf.evaluate(partial_x_train, partial_y_train)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-9e077cfb15d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mge_flat_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial_x_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartial_y_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ge_flat_clf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbirchv7lord",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "7710462a-8cdc-4f9a-f774-c7c09cec616c"
      },
      "source": [
        "flat_art_clf = KerasClassifier(model=ge_flat_clf, clip_values=(0, 1), use_logits=False)\n",
        "\n",
        "attack_FEN = FunctionallyEquivalentExtraction(classifier=flat_art_clf, num_neurons=100)\n",
        "stolen_clf_FEN = attack_FEN.extract(partial_x_train, partial_y_train,\n",
        "                                    delta_0=0.05,\n",
        "                                    fraction_true=0.3,\n",
        "                                    rel_diff_slope=0.1,\n",
        "                                    rel_diff_value=0.1,\n",
        "                                    delta_init_value=0.1,\n",
        "                                    delta_value_max=2,\n",
        "                                    d2_min=0.04,\n",
        "                                    d_step=0.01,\n",
        "                                    delta_sign=0.2,\n",
        "                                    unit_vector_scale=1000)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-2fb0c93650e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mflat_art_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mge_flat_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mattack_FEN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionallyEquivalentExtraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_art_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_neurons\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m stolen_clf_FEN = attack_FEN.extract(partial_x_train, partial_y_train,\n\u001b[1;32m      5\u001b[0m                                     \u001b[0mdelta_0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ge_flat_clf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUuxY57blow6"
      },
      "source": [
        "stolen_clf_FEN._model.evaluate(partial_x_train, partial_y_train)\n",
        "\n",
        "### Evaluation\n",
        "stolen_clf_FEN._model.evaluate(x_train_adv, y_train_adv)\n",
        "stolen_clf_FEN._model.evaluate(x_test_adv, y_test_adv)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}